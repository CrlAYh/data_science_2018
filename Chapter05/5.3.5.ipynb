{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "import matplotlib\n",
    "import mglearn\n",
    "import os\n",
    "\n",
    "from preamble import *\n",
    "#plt.rcParams['image.cmap'] = \"gray\"\n",
    "\n",
    "from matplotlib.pyplot import rc\n",
    "rc('font',family='New Gulim') #plot 한글폰트 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "# 5.2\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#5.3\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from mglearn.datasets import make_blobs \n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.5 모델 선택에서 평가 지표 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 선택에서 평가 지표 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기본 평가 지표: [0.9 0.9 0.9]\n",
      "정확도 지표: [0.9 0.9 0.9]\n",
      "AUC 지표: [0.994 0.99  0.996]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision scoring: [0.81 0.81 0.81]\n",
      "Precision scoring: [0.9 0.9 0.9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score scoring: [0.852 0.852 0.852]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# 분류의 기본 평가 지표는 정확도 입니다\n",
    "print(\"기본 평가 지표: {}\".format(\n",
    "      cross_val_score(SVC(), digits.data, digits.target == 9)))\n",
    "# scoring=\"accuracy\"의 결과는 같습니다.\n",
    "explicit_accuracy =  cross_val_score(SVC(), digits.data, digits.target == 9,\n",
    "                                     scoring=\"accuracy\")\n",
    "print(\"정확도 지표: {}\".format(explicit_accuracy))\n",
    "roc_auc =  cross_val_score(SVC(), digits.data, digits.target == 9,\n",
    "                           scoring=\"roc_auc\")\n",
    "print(\"AUC 지표: {}\".format(roc_auc))\n",
    "\n",
    "print()\n",
    "# 다양한 성능 측정 (Not Recommended)\n",
    "precision = cross_val_score(SVC(), digits.data, digits.target == 9, scoring=\"precision_weighted\")\n",
    "print(\"Precision scoring: {}\".format(precision))\n",
    "\n",
    "recall = cross_val_score(SVC(), digits.data, digits.target == 9, scoring='recall_weighted')\n",
    "print(\"Precision scoring: {}\".format(recall))\n",
    "\n",
    "f1_score = cross_val_score(SVC(), digits.data, digits.target == 9, scoring='f1_weighted')\n",
    "print(\"F1_score scoring: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 지표를 사용한 그리드 서치\n",
      "최적의 파라미터: {'gamma': 0.0001}\n",
      "최상의 교차 검증 점수 (정확도)): 0.970\n",
      "테스트 세트 AUC: 0.992\n",
      "테스트 세트 accuracy: 0.973\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    digits.data, digits.target == 9, random_state=0)\n",
    "\n",
    "# 일부러 적절하지 않은 그리드를 만듭니다\n",
    "param_grid = {'gamma': [0.0001, 0.01, 0.1, 1, 10]}\n",
    "# 기본 정확도 측정 지표를 사용합니다\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"정확도 지표를 사용한 그리드 서치\")\n",
    "print(\"최적의 파라미터:\", grid.best_params_)\n",
    "print(\"최상의 교차 검증 점수 (정확도)): {:.3f}\".format(grid.best_score_))\n",
    "print(\"테스트 세트 AUC: {:.3f}\".format(\n",
    "        roc_auc_score(y_test, grid.decision_function(X_test))))\n",
    "print(\"테스트 세트 accuracy: {:.3f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC 지표를 사용한 그리드 서치\n",
      "최적의 파라미터: {'gamma': 0.01}\n",
      "최상의 교차 검증 점수 (AUC): 0.997\n",
      "테스트 세트 AUC: 1.000\n"
     ]
    }
   ],
   "source": [
    "# AUC 지표 사용\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, scoring=\"roc_auc\")\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"AUC 지표를 사용한 그리드 서치\")\n",
    "print(\"최적의 파라미터:\", grid.best_params_)\n",
    "print(\"최상의 교차 검증 점수 (AUC): {:.3f}\".format(grid.best_score_))\n",
    "print(\"테스트 세트 AUC: {:.3f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가능한 평가 방식:\n",
      "['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'log_loss', 'mean_absolute_error', 'mean_squared_error', 'median_absolute_error', 'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.scorer import SCORERS\n",
    "print(\"가능한 평가 방식:\\n{}\".format(sorted(SCORERS.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
